\documentclass[11pt]{article}
\input{preamble/preamble}
\input{preamble/math-preamble.tex}

\begin{document}

\title{  {\LARGE STAT 578 Final Project: Rate-Optimal Graphon Estimation} }

\author{
Joshua Loyal \,
Mauricio Campos
}

\date{\today}
\maketitle

% Introduction
\section{Introduction} \label{sec:intro}

Many scientific fields involve the analysis of network data. Applications include social networks, networks in statistical mechanics, biological networks, and information networks \citep{goldenberg2010survey}. As a result the statistics and machine learning communities have developed a plethora of methods for understanding networks in recent years. A large collection of these methods involve the nonparametric estimation of a special function known as a graphon \citep{borgs2008graphon, chatterjee2015univsvd, chan2014histo}. Just as in nonparametric function estimation with i.i.d data and a fixed design, it is important to known the optimal rate of convergence of such nonparametric graphon estimators. Such a rate allows one to determine whether certain proposed estimating procedures can be improved upon. In the remainder of this paper we will review the article by Gao and Zhu \citep{gao2015optgraphon}, which derives this minimax optimal rate for nonparameteric graphon estimation. In fact, there are two rates of convergence that we will cover. Each rate corresponds to certain assumptions made about the underlying graph generating process. The first rate pertains to estimation under the stochastic block model. The second rate only assumes that the graphon function belongs to an appropriate smoothness class.

% Notation and Assumptions
\section{Notation and Assumptions} \label{sec:notation}

We will begin by outlining the details of the estimation problem. We consider an undirected graph with $n$ nodes and no self-loops. The edges of the graph are encoded in a binary matrix $\set{A_{ij}} \in \set{0, 1}^{n \times n}$ known as an adjacency matrix. Note that the $\Adj$ entry of the adjacency matrix is 1 if node $i$ and $j$ are connected in the underlying graph and 0 otherwise. Since the graph is undirected, the adjacency matrix is symmetric, i.e. $A_{ij} = A_{ji}$. The absence of self-loops implies $A_{ii} = 0$. The underlying probabilistic model for the adjacency matrix is
\begin{equation}\label{eq:graph_model}
\begin{aligned}
\xi_i &\overset{\text{iid}}{\sim} \GraphonDist, \quad i \in \brackets{n} \\
\theta_{ij} &= f(\xi_{i}, \xi_{j}), \quad i \neq j \in \brackets{n} \\
A_{ij} &= A_{ji} \sim \Bern(\theta_{ij}).
\end{aligned}
\end{equation}
where $\GraphonDist$ is a distribution supported on $[0, 1]^n$. Often, $\GraphonDist$ is assumed to be a uniform distribution on the unit interval; however, the results in the paper do not rely on this restriction. A key assumption of this model is that entries of the adjacency matrix $\set{A_{ij}}$ are independent given a collection of unobserved pairs of random variables $\latentvars$. Furthermore, the latent variables are linked to the probability that nodes $i$ and $j$ form an edge through a symmetric function $\graphon$ known as a graphon. This graphon function is the primary object of interest under the model in \ref{eq:graph_model}. Knowledge of this object allows one to compare two different networks or to make predictions about future links in the network.
For this reason, the goal of the estimation problem is to estimate $\graphon$ given the observed adjacency matrix, but with an unobserved design due to the presence of the latent variables $\latentvars$.

Gao and Zhu consider estimation of $\graphon$ under a squared error loss. This loss function leads to the following empirical risk minimization criteria:
\begin{equation}\label{eq:loss_func}
\frac{1}{n^2}\sum_{i, j \in \brackets{n}}(\hat{f}(\xi_i, \xi_j) - f(\xi_i, \xi_j))^2 =
\frac{1}{n^2}\sum_{i, j \in \brackets{n}}(\hat{\theta}_{ij} - \theta_{ij})^2
\end{equation}
Note that estimating $\graphon$ is equivalent to estimating each pairwise probability $\edgeproba$. Due to the unobserved design, this estimation problem is only made possible by imposing certain structure on the connection probabilities. In this paper they consider two widely used restrictions: the stochastic block model and the assumption that $\graphon$ belongs to a H\"older class $\holder$ with smoothness $\alpha$. To describe the restrictions in detail we need some notation.

 Let $\partitions = \set{z : \brackets{n} \rightarrow \brackets{k}}$ be the collection of all possible $k^n$ partition mappings (order matters in the mapping) of the numbers $1, \dots, n$. In particular, the sets $\set{z^{-1}(a) : a \in \brackets{k}}$ form a partition of $\brackets{n}$, meaning that $\bigcup_{a \in \brackets{k}} z^{-1}(a) = \brackets{n}$ and $z^{-1}(a) \cap z^{-1}(b) = \emptyset$ for any $a \neq b$. Additionally, given a matrix $\set{\eta_{ij}} \in \Reals{n\times n}$, and a partition function $z \in \partitions$, define the block average on the set $z^{-1}(a) \times z^{-1}(b)$ as
 
\begin{equation}
\begin{aligned}
\bar{\eta}_{ab} = \frac{1}{\abs{z^{-1}(a)}\abs{z^{-1}(b)}} \sum_{i \in z^{-1}(a)} \sum_{j \in z^{-1}(b)} \eta_{ij} \quad \text{for } a \neq b \in \brackets{k}
\end{aligned}
\end{equation}

and when $\abs{z^{-1}(a)} > 1$,

\begin{equation}
\begin{aligned}
\bar{\eta}_{aa} = \frac{1}{\abs{z^{-1}(a)}\left( \abs{z^{-1}(a)}-1 \right)} \sum_{i \neq j \in z^{-1}(a)}  \eta_{ij} \quad \text{for } a \in \brackets{k}
\end{aligned}
\end{equation}

In more generality, for any $Q = \set{Q}\in \Reals{k\times k}$ and $z \in \partitions$, define the objective function

\[ L(Q,z) = \sum_{a,b\in\brackets{k}} \sum_{(i,j)\in z^{-1}(a) \times z^{-1}(b)} \left( A_{ij} - Q{ab} \right)^2  \]

For any optimizer of the objective function,

\begin{equation}
\begin{aligned}
( \hat{Q}, \hat{z} ) \in \argmin_{Q \in \Reals{k \times k}, z \in \partitions}  L(Q,z)
\end{aligned}
\end{equation}

the estimator of $\theta_{ij}$ is defined as

\begin{equation}
\begin{aligned}
\htheta_{ij} = \hat{Q}_{\hat{z}(i)\hat{z}(j)}, \qquad i > j,
\end{aligned}
\end{equation}

and $\htheta_{ij} = \htheta_{ji}$ for $i<j$. For the diagonal element it is set as $\htheta_{ii} = 0$. This can be seen as first clustering the data using an estimated partition function and then estimating the model parameters by using block averages. From the least squares formulation it is easy to observe that $\hat{Q}_{ab} = \bar{A}_{ab}(\hat{z}), \forall a,b \in \brackets{k}$, for any minimizer $( \hat{Q}, \hat{z} )$. In other words, the estimator $\htheta_{ij}$ is essentially a histogram approximation after finding the optimal cluster assignment according to the least square criterion.

In more detail, the two restrictions previously mentioned are as follows:

\begin{assump}\label{assump:sbm}
\textit{Stochastic Block Model with k clusters or SBM($k$)}

\noindent
The SBM($k$) assumes that the edges are partitioned into $k$ clusters with different probabilities assigned to a connection between members sharing a cluster assignment and those in different clusters. This means that there are $k^2$ different connection probabilities in the model. Denote by $\set{Q_{ab}} \in [0, 1]^{k \times k}$ the matrix whose $ab$ entry is the probability that cluster $a$ and $b$ form a connection. However, since the nodes are not ordered by cluster, the possible values of $\edgeprobas$ must allow for all possible permutations. This is captured in the following restricted parameter space for $\edgeprobas$:
\begin{equation}
\begin{aligned}
\Theta_k = \set{&\edgeprobas \in [0, 1]^{n \times n} : \theta_{ii} = 0, \edgeproba = Q_{ab} = Q_{ba}\\
    & \text{for} \, (i, j) \in z^{-1}(a) \times z^{-1}(b) \, \text{for some} \, Q_{ab} \in [0, 1] \, \text{and} \, z \in \partitions}
\end{aligned}
\end{equation}
\end{assump}


\begin{assump}\label{assump:holder}
\textit{Nonparametric graphon estimation in $\holder$}

\noindent
The second assumption requires the definition of an appropriate H\"older class for bi-variate functions (in class we just discussed H\"older classes for univariate functions). Since $\graphon$ is symmetric, without loss of generality we restrict the behavior of $f$ for $x \geq y$. Let $\mathcal{D} = \set{(x, y) \in [0, 1]^2 : x \geq y}$ Define the mixed partial derivative of order $j + k$ as
\begin{equation}
\nabla_{jk} f(x, y) = \frac{\partial^{j+k}}{(\partial x)^j (\partial y)^k}f(x, y)
\end{equation}
where $\nabla_{00} f(x, y) = f(x, y)$. The H\"older norm on $\mathcal{D}$ is then defined as
\begin{equation}
\begin{aligned}
\norm{f}_{\mathcal{F}_{\alpha}} = &\max_{j + k \leq \floor{\alpha}} \sup_{x,y \in \mathcal{D}} \abs{\nabla_{jk}f(x, y)} \ + \\
& \max_{j + k = \floor{\alpha}} \sup_{(x, y) \neq (x', y') \in \mathcal{D}}
\frac{\abs{\nabla_{jk}f(x,y) - \nabla_{jk}f(x', y')}}{(\abs{x - x'} + \abs{y -y'})^{\alpha - \floor{\alpha}}}
\end{aligned}
\end{equation}
We then restrict $\graphon$ to the following H\"older class:
\begin{equation}
\holder = \set{f \in [0, 1] : \norm{f}_{\mathcal{F}_{\alpha}} \leq M \text{ and } f(x, y) = f(y, x) \text{ for } x \geq y}
\end{equation}
\end{assump}

Before stating the key results of the paper, it is worth pointing out the behavior of $\holder$ functions when $\alpha \in (0, 1]$. It is easy to see that this regime of smoothness parameter implies the Liptchitz condition
\begin{equation}
\abs{f(x,y) - f(x', y')} \leq M (\abs{x - x'} + \abs{y - y'})^{\alpha}.
\end{equation}
Such a nice interpretation is important because we will later see that this regime of smoothness parameter results in a minimax rate that mimics the well known minimax rate for nonparametric regression with a fixed design.

\section{Key Results} \label{sec:key_results}

The main contributions of the paper are deriving the minimax rates of estimation for Assumption \ref{assump:sbm} and Assumption \ref{assump:holder}. This is done by deriving the lower and upper bounds. The results are summarized in the following two theorems:

\begin{theorem} \label{thm:sbm}
Under the SBM($k$), we have
\begin{equation}
\inf_{\hat{\theta}} \sup_{\theta \in \Theta_k} \mathbb{E}\left\{\frac{1}{n^2} \sum_{i,j \in \brackets{n}} (\hat{\theta}_{ij} - \theta_{ij})^2\right\} \asymp \frac{k^2}{n^2} + \frac{\log k}{n}
\end{equation}
for any $1 \leq k \leq n$.
\end{theorem}

\begin{theorem}\label{thm:nonparam}
Consider the H\"older class $\holder$. We have
\begin{equation}
\inf_{\hat{\theta}} \sup_{f \in \holder} \sup_{\xi \sim \Dist_{\xi}}\mathbb{E}\left\{\frac{1}{n^2} \sum_{i,j \in \brackets{n}} (\hat{\theta}_{ij} - \theta_{ij})^2\right\} \asymp
\begin{cases}
n^{-2\alpha/(\alpha + 1)}, \quad 0 \leq \alpha \leq 1 \\
\frac{\log n}{n}, \quad \alpha \geq 1,
\end{cases}
\end{equation}
where the expectation is jointly over $\Adj$ and $\set{\xi_i}$.
\end{theorem}

Before moving onto the proofs, it is worth discussing an important decomposition of the rates in Theorem \ref{thm:sbm} and \ref{thm:nonparam} pointed out by Gao and Zhu. They breakdown the rates into the \textit{clustering rate} and \textit{nonparametric rate}. Intuitively these rates come about due to the two types of quantities one has to estimate in the graphon problem. At a high level the clustering rate comes about due to the unknown design of the latent variables $\xi_i$. In the SBM($k$) this is equivalent to the estimation of the group assignments of the nodes. This is the origin of the $n^{-1} \log k$ term in Theorem \ref{thm:sbm} and the $n^{-1} \log n$ term in Theorem \ref{thm:nonparam}. On the other hand the nonparametric rate is due to the estimation of the nonparametric graphon function $\graphon$. In the SBM($k$) this is equivalent to estimating the cluster probability matrix $\set{Q_{ab}} \in [0, 1]^{k \times k}$. This results in the $k^2 /n^2$ and the $n^{-2\alpha/(\alpha + 1)}$ terms in the theorems. The difficulty of the problem is then due to the difficulty of these two subproblems. Characterizing the difficulty of these subproblems is essentially the crux of proving the lower bounds in these theorems.

\section{Proof Outlines}\label{sec:proofs}

\subsection{Upper Bound SBM($k$)} \label{sec:upper_sbm}



\subsection{Lower Bound SBM($k$)} \label{sec:lower_sbm}
Note that the lower bound of the expectation in Theorem \ref{thm:sbm} is equivalent to a lower bound of the following probability (Markov Inequality):
\begin{equation}
\begin{aligned}
&\inf_{\hat{\theta}} \sup_{\theta \in \Theta_k} \Dist \left\{ \frac{1}{n^2}\sum_{i,j \in \brackets{n}} (\hat{\theta}_{ij} - \theta_{ij})^2 \geq \epsilon^2\right\}  \\
&= \inf_{\hat{\theta}} \sup_{Q = Q^T \in [0, 1]^{k \times k}} \sup_{z \in \partitions} \Dist \left\{ \frac{1}{n^2}\sum_{i,j \in \brackets{n}} (\hat{\theta}_{ij} - Q_{z(i), z(j)})^2 \geq \epsilon^2\right\}
\end{aligned}
\end{equation}
for any $\epsilon > 0$. We can now see how the clustering and nonparametric rate come into the bound. We have a supremum over both the group probabilities $Q$ and the cluster assignments $z$. Just as in class, the proof of the in-probability lower bound is an application of Fano's Lemma. To apply Fano's Lemma we restrict the supremum of $\Theta_k$ to a subset $T \subset \Theta_k$ with a finite packing number $\mathcal{M}(2 \epsilon, T, \rho)$. In this case we can restrict $\Theta_k$ in two different ways. The first way fixes $z \in \partitions$ and then finds an appropriate packing as $Q$ varies. This leads to the nonparametric rate. The second way fixes $Q$ and finds an appropriate packing as $z$ varies. This leads to the clustering rate. As a result we will have two lower bounds, which we can then combine using what is essentially a union bound argument. We start with the nonparametric rate.

\textit{Nonparametric rate:} The nonparametric rate actually utilizes a version of Fano's Lemma in terms of the $\chi^2$ distance instead of the Kullback-Leibler distance. The statement is as follows:
\begin{equation}\label{eq:chi2_fano}
\inf_{\hat{\theta}} \sup_{\theta \in \Theta} \Dist_{\theta} \left\{ \rho^2(\hat{\theta}, \theta)  \geq \frac{\epsilon^2}{4} \right\} \geq 1 - \frac{1}{\packing} - \sqrt{\frac{\dchisq{T}}{\packing}}.
\end{equation}
where $\dchisq{T} = \sup_{\theta, \theta'}\chi^2(\Dist_{\theta} || \Dist_{\theta'})$. Furthermore, note that $\rho(\theta, \theta') = \frac{1}{n^2} \sum_{ij}(\theta_{ij} - \theta_{ij}')^2$ and $\Dist_{\theta} = \bigotimes_{i,j \in \brackets{n}} \Bern(\theta_{ij})$. Thus, the proof has two steps: for an appropriate subset $T$ we need to 1. upper bound $\dchisq{T}$ and 2. lower bound the packing number $\packing$. Since $\Dist_{\theta}$ is just a collection of independent Bernoulli's it is straightforward to bound $\chi^2(\Dist_{\theta} || \Dist_{\theta'})$:
\begin{align*}
\chi^2(\Dist_{\theta} || \Dist_{\theta'}) &= \prod_{ij}\left( \theta_{ij}' \left(\frac{\theta_{ij}}{\theta_{ij}'}\right)^2  +  (1 - \theta_{ij}') \left(\frac{1 - \theta_{ij}}{1 - \theta_{ij}'}\right)^2\right)  - 1 \\
&= \prod_{ij}\left(1 - \frac{(\theta_{ij} - \theta_{ij}')^2}{\theta_{ij}' ( 1- \theta_{ij}')}\right) - 1 \\
&\leq \exp\left(\sum_{ij} \log(1 - 8 (\theta_{ij} - \theta_{ij}')^2)\right) - 1 \\
&\leq \exp\left(8 \sum_{ij} (\theta_{ij} - \theta_{ij}')^2 \right)
\end{align*}
where we are using the fact that $\log(x) \leq x - 1$ for $x > 0$ and we will latter have to restrict $T$ so that $\theta_{ij}' \in [1/4, 3/4]$.

To proceed further we need to find an appropriate subset $T$. Gao and Zhu use the following construction, which relies on a Varshamov-Gilbert bound to obtain an appropriate packing. Recall for this rate we are free to choose $z$. A simple choice is a balanced partitioning. Without loss of generality we can assume $n / k$ and $k / 2$ are integers. A balanced partition then sets the partition mapping for each $a \in \brackets{k}$ as $z^{-1}(a) = \set{(a-1)n/k + 1, \dots, an/k}$. Note that there are $n/k$ nodes assigned to each cluster. We can then define $Q^{\omega} = (Q_{ab}^{\omega})_{k \times k}$ by
\begin{equation}
\begin{aligned}
Q^{\omega}_{ab} &= Q^{\omega}_{ba} = \frac{1}{2} + \frac{c_1 k}{n} \omega_{ab} \quad \text{for } a > b \in \brackets{k}, \\
Q_{aa}^{\omega} &= \frac{1}{2} \quad \text{for } a \in \brackets{k}.
\end{aligned}
\end{equation}
where $\omega_{ab} \in \HyperCube$ with $d = k(k-1)/2$ (number of entries we need to fix in $Q_{ab}$), and $c_1$ is a constant introduced to satisfy $\theta_{ij}' \in [1/4, 3/4]$ in the bound of the $\chi^2$ distance. We are now ready to define our subset $T$. Let $\theta^{\omega} = (\theta_{ij}^{\omega})_{n \times n}$ with $\theta_{ij}^{\omega} = Q_{z(i)z(j)}^{\omega}$ for $i \neq j$ and $\theta_{ii}^{\omega} = 0$. Then let $T_1 = \set{\theta^{\omega} : \omega \in \HyperCube}$. The bound on the $\chi^2$ distance is straightforward:
\begin{equation}
\begin{aligned}
\chi^2(\Dist_{\theta^{\omega}} || \Dist_{\theta^{\omega'}}) &\leq \exp\left(8 \sum_{ij} (\theta_{ij}^{\omega} - \theta_{ij}^{\omega'})^2\right) \\
&= \exp\left(\frac{8 n^2}{k^2} \sum_{ab} (Q_{ab}^{\omega} - Q_{ab}^{\omega'})^2\right)
\leq \exp(8 c_1^2 k^2).
\end{aligned}
\end{equation}
where $c_1$ is chosen small enough. We lower bound the packing number through the VG Lemma, i.e.
we can find a subset $S \subset \HyperCube$ such that 1. $|S| \geq \exp(d/8)$ and 2. $\rho_{H}(\omega, \omega') \geq d/4$. Thus,
\begin{equation}
\rho^2(\theta^{\omega}, \theta^{\omega'}) \geq \frac{1}{k^2} \sum_{1 \leq b < a \leq k} (Q_{ab}^{\omega} - Q_{ab}^{\omega'})^2 = \frac{c_1^2}{n^2} \rho_{H}(\omega, \omega').\geq \frac{c_1^2 d}{4 n^2} = .\frac{c_1^2 k(k-1)}{8 n^2}
\end{equation}
This implies
\begin{equation}
\mathcal{M}\left(\frac{c_1^2 k(k-1)}{8 n^2}, T_1, \rho\right) \geq |S| \geq \exp(d/8).
\end{equation}
We can then combine these bounds with Equation \ref{eq:chi2_fano} to conclude that with a constant $C_1$ chosen large enough
\begin{equation}
\inf_{\hat{\theta}} \sup_{\theta \in T_1} \Dist \left\{\frac{1}{n^2} \sum_{ij} (\hat{\theta}_{ij} - \theta_{ij})^2 \geq \frac{C_1 k^2}{n^2} \right\} \geq 0.9.
\end{equation}

\textit{Clustering rate:}


\subsection{Upper Bound H\"older Class} \label{sec:upper_hold}

\subsection{Lower Bound H\"older Class} \label{sec:lower_hold}

\bibliographystyle{asa}
\bibliography{reference}

\end{document}
