\documentclass[11pt]{article}
\input{preamble/preamble}
\input{preamble/math-preamble.tex}

\begin{document}

\title{  {\LARGE STAT 578 Final Project: Rate-Optimal Graphon Estimation} }

\author{
Joshua Loyal \,
Mauricio Campos
}

\date{\today}
\maketitle

% Introduction
\section{Introduction} \label{sec:intro}

Many scientific fields involve the analysis of network data. Applications include social networks, networks in statistical mechanics, biological networks, and information networks \citep{goldenberg2010survey}. As a result the statistics and machine learning communities have developed a plethora of methods for understanding networks in recent years. A large collection of these methods involve the nonparametric estimation of a special function known as a graphon \citep{borgs2008graphon, chatterjee2015univsvd, chan2014histo}. Just as in nonparametric function estimation with i.i.d data and a fixed design, it is important to known the optimal rate of convergence of such nonparametric graphon estimators. Such a rate allows one to determine whether certain proposed estimating procedures can be improved upon. In the remainder of this paper we will review the article by Gao and Zhu \citep{gao2015optgraphon}, which derives this minimax optimal rate for nonparameteric graphon estimation. In fact, there are two rates of convergence that we will cover. Each rate corresponds to certain assumptions made about the underlying graph generating process. The first rate pertains to estimation under the stochastic block model. The second rate only assumes that the graphon function belongs to an appropriate smoothness class.

% Notation and Assumptions
\section{Notation and Assumptions} \label{sec:notation}

We will begin by outlining the details of the estimation problem. We consider an undirected graph with $n$ nodes and no self-loops. The edges of the graph are encoded in a binary matrix $\set{A_{ij}} \in \set{0, 1}^{n \times n}$ known as an adjacency matrix. Note that the $\Adj$ entry of the adjacency matrix is 1 if node $i$ and $j$ are connected in the underlying graph and 0 otherwise. Since the graph is undirected, the adjacency matrix is symmetric, i.e. $A_{ij} = A_{ji}$. The absence of self-loops implies $A_{ii} = 0$. The underlying probabilistic model for the adjacency matrix is
\begin{equation}\label{eq:graph_model}
\begin{aligned}
\xi_i &\overset{\text{iid}}{\sim} \GraphonDist, \quad i \in \brackets{n} \\
\theta_{ij} &= f(\xi_{i}, \xi_{j}), \quad i \neq j \in \brackets{n} \\
A_{ij} &= A_{ji} \sim \Bern(\theta_{ij}).
\end{aligned}
\end{equation}
where $\GraphonDist$ is a distribution supported on $[0, 1]^n$. Often, $\GraphonDist$ is assumed to be a uniform distribution on the unit interval; however, the results in the paper do not rely on this restriction. A key assumption of this model is that entries of the adjacency matrix $\set{A_{ij}}$ are independent given a collection of unobserved pairs of random variables $\latentvars$. Furthermore, the latent variables are linked to the probability that nodes $i$ and $j$ form an edge through a symmetric function $\graphon$ known as a graphon. This graphon function is the primary object of interest under the model in \ref{eq:graph_model}. Knowledge of this object allows one to compare two different networks or to make predictions about future links in the network.
For this reason, the goal of the estimation problem is to estimate $\graphon$ given the observed adjacency matrix, but with an unobserved design due to the presence of the latent variables $\latentvars$.

Gao and Zhu consider estimation of $\graphon$ under a squared error loss. This loss function leads to the following empirical risk minimization criteria:
\begin{equation}\label{eq:loss_func}
\frac{1}{n^2}\sum_{i, j \in \brackets{n}}(\hat{f}(\xi_i, \xi_j) - f(\xi_i, \xi_j))^2 =
\frac{1}{n^2}\sum_{i, j \in \brackets{n}}(\hat{\theta}_{ij} - \theta_{ij})^2
\end{equation}
Note that estimating $\graphon$ is equivalent to estimating each pairwise probability $\edgeproba$. Due to the unobserved design, this estimation problem is only made possible by imposing certain structure on the connection probabilities. In this paper they consider two widely used restrictions: the stochastic block model and the assumption that $\graphon$ belongs to a H\"older class $\holder$ with smoothness $\alpha$. To describe the restrictions in detail we need some notation. Let $\partitions = \set{z : \brackets{n} \rightarrow \brackets{k}}$ be the collection of all possible $k^n$ partition mappings (order matters in the mapping) of the numbers $1, \dots, n$. In particular, the sets $\set{z^{-1}(a) : a \in \brackets{k}}$ form a partition of $\brackets{n}$. In more detail, the two restrictions are as follows:

\begin{assump}\label{assump:sbm}
\textit{Stochastic Block Model with k clusters or SBM($k$)}

\noindent
The SBM($k$) assumes that the edges are partitioned into $k$ clusters with different probabilities assigned to a connection between members sharing a cluster assignment and those in different clusters. This means that there are $k^2$ different connection probabilities in the model. Denote by $\set{Q_{ab}} \in [0, 1]^{k \times k}$ the matrix whose $ab$ entry is the probability that cluster $a$ and $b$ form a connection. However, since the nodes are not ordered by cluster, the possible values of $\edgeprobas$ must allow for all possible permutations. This is captured in the following restricted parameter space for $\edgeprobas$:
\begin{equation}
\begin{aligned}
\Theta_k = \set{&\edgeprobas \in [0, 1]^{n \times n} : \theta_{ii} = 0, \edgeproba = Q_{ab} = Q_{ba}\\
    & \text{for} \, (i, j) \in z^{-1}(a) \times z^{-1}(b) \, \text{for some} \, Q_{ab} \in [0, 1] \, \text{and} \, z \in \partitions}
\end{aligned}
\end{equation}
\end{assump}


\begin{assump}\label{assump:holder}
\textit{Nonparametric graphon estimation in $\holder$}

\noindent
The second assumption requires the definition of an appropriate H\"older class for bi-variate functions (in class we just discussed H\"older classes for univariate functions). Since $\graphon$ is symmetric, without loss of generality we restrict the behavior of $f$ for $x \geq y$. Let $\mathcal{D} = \set{(x, y) \in [0, 1]^2 : x \geq y}$ Define the mixed partial derivative of order $j + k$ as
\begin{equation}
\nabla_{jk} f(x, y) = \frac{\partial^{j+k}}{(\partial x)^j (\partial y)^k}f(x, y)
\end{equation}
where $\nabla_{00} f(x, y) = f(x, y)$. The H\"older norm on $\mathcal{D}$ is then defined as
\begin{equation}
\begin{aligned}
\norm{f}_{\mathcal{F}_{\alpha}} = &\max_{j + k \leq \floor{\alpha}} \sup_{x,y \in \mathcal{D}} \abs{\nabla_{jk}f(x, y)} \ + \\
& \max_{j + k = \floor{\alpha}} \sup_{(x, y) \neq (x', y') \in \mathcal{D}}
\frac{\abs{\nabla_{jk}f(x,y) - \nabla_{jk}f(x', y')}}{(\abs{x - x'} + \abs{y -y'})^{\alpha - \floor{\alpha}}}
\end{aligned}
\end{equation}
We then restrict $\graphon$ to the following H\"older class:
\begin{equation}
\holder = \set{f \in [0, 1] : \norm{f}_{\mathcal{F}_{\alpha}} \leq M \text{ and } f(x, y) = f(y, x) \text{ for } x \geq y}
\end{equation}
\end{assump}

Before stating the key results of the paper, it is worth pointing out the behavior of $\holder$ functions when $\alpha \in (0, 1]$. It is easy to see that this regime of smoothness parameter implies the Liptchitz condition
\begin{equation}
\abs{f(x,y) - f(x', y')} \leq M (\abs{x - x'} + \abs{y - y'})^{\alpha}.
\end{equation}
Such a nice interpretation is important because we will later see that this regime of smoothness parameter results in a minimax rate that mimics the well known minimax rate for nonparametric regression with a fixed design.

\section{Key Results} \label{sec:key_results}

The main contributions of the paper are deriving the minimax rates of estimation for Assumption \ref{assump:sbm} and Assumption \ref{assump:holder}. This is done by deriving the lower and upper bounds. The results are summarized in the following two theorems:

\begin{theorem} \label{thm:sbm}
Under the SBM($k$), we have
\begin{equation}
\inf_{\hat{\theta}} \sup_{\theta \in \Theta_k} \mathbb{E}\left\{\frac{1}{n^2} \sum_{i,j \in \brackets{n}} (\hat{\theta}_{ij} - \theta_{ij})^2\right\} \asymp \frac{k^2}{n^2} + \frac{\log k}{n}
\end{equation}
for any $1 \leq k \leq n$.
\end{theorem}

\begin{theorem}\label{thm:nonparam}
Consider the H\"older class $\holder$. We have
\begin{equation}
\inf_{\hat{\theta}} \sup_{f \in \holder} \sup_{\xi \sim \Dist_{\xi}}\mathbb{E}\left\{\frac{1}{n^2} \sum_{i,j \in \brackets{n}} (\hat{\theta}_{ij} - \theta_{ij})^2\right\} \asymp
\begin{cases}
n^{-2\alpha/(\alpha + 1)}, \quad 0 \leq \alpha \leq 1 \\
\frac{\log n}{n}, \quad \alpha \geq 1,
\end{cases}
\end{equation}
where the expectation is jointly over $\Adj$ and $\set{\xi_i}$.
\end{theorem}

Before moving onto the proofs, it is worth discussing an important decomposition of the rates in Theorem \ref{thm:sbm} and \ref{thm:nonparam} pointed out by Gao and Zhu. They breakdown the rates into the \textit{clustering rate} and \textit{nonparametric rate}. Intuitively these rates come about due to the two types of quantities one has to estimate in the graphon problem. At a high level the clustering rate comes about due to the unknown design of the latent variables $\xi_i$. In the SBM($k$) this is equivalent to the estimation of the group assignments of the nodes. This is the origin of the $n^{-1} \log k$ term in Theorem \ref{thm:sbm} and the $n^{-1} \log n$ term in Theorem \ref{thm:nonparam}. On the other hand the nonparametric rate is due to the estimation of the nonparametric graphon function $\graphon$. In the SBM($k$) this is equivalent to estimating the cluster probability matrix $\set{Q_{ab}} \in [0, 1]^{k \times k}$. This results in the $k^2 /n^2$ and the $n^{-2\alpha/(\alpha + 1)}$ terms in the theorems. The difficulty of the problem is then due to the difficulty of these two subproblems. Characterizing the difficulty of these subproblems is essentially the crux of proving the lower bounds in these theorems.

\section{Proof Outlines}\label{sec:proofs}

\subsection{Upper Bound SBM($k$)} \label{sec:upper_sbm}



\subsection{Lower Bound SBM($k$)} \label{sec:lower_sbm}

\subsection{Upper Bound H\"older Class} \label{sec:upper_hold}

\subsection{Lower Bound H\"older Class} \label{sec:lower_hold}

\bibliographystyle{asa}
\bibliography{reference}

\end{document}
